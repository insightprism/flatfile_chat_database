{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatfile Chat Database - Interactive Demo\n",
    "\n",
    "Welcome to the comprehensive demo of the Flatfile Chat Database System! This notebook will walk you through all the key features of this file-based storage solution for AI chat applications.\n",
    "\n",
    "## What you'll learn:\n",
    "- ðŸ’¾ **Chat Storage**: Store messages, sessions, and user profiles\n",
    "- ðŸ“„ **Document Processing**: Add documents and create embeddings\n",
    "- ðŸ” **Vector Search**: Semantic similarity search\n",
    "- ðŸ”Ž **Advanced Search**: Text-based search with filters\n",
    "- âš™ï¸ **Configuration**: Both legacy and new architecture\n",
    "- ðŸ§  **PrismMind Integration**: Enhanced document processing\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added PrismMind path: /home/markly2/prismmind\n",
      "âœ… All imports successful\\!\n",
      "ðŸ“ Working directory: /home/markly2/claude_code/flatfile_chat_database_v2/demo\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Add parent directory to path to import the flatfile database\n",
    "sys.path.append('..')\n",
    "\n",
    "# Add PrismMind directory to path (if available)\n",
    "prismmind_path = '/home/markly2/prismmind'\n",
    "if os.path.exists(prismmind_path):\n",
    "    sys.path.append(prismmind_path)\n",
    "    print(f\"âœ… Added PrismMind path: {prismmind_path}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ PrismMind not found at: {prismmind_path}\")\n",
    "\n",
    "# Import the main components - NO LEGACY ADAPTER\n",
    "from ff_storage_manager import FFStorageManager\n",
    "from ff_class_configs.ff_configuration_manager_config import FFConfigurationManagerConfigDTO, load_config\n",
    "from ff_class_configs.ff_chat_entities_config import (\n",
    "    FFMessageDTO, FFSessionDTO, FFUserProfileDTO, MessageRole\n",
    ")\n",
    "from ff_search_manager import FFSearchManager, FFSearchQueryDTO\n",
    "from ff_vector_storage_manager import FFVectorStorageManager\n",
    "from ff_document_processing_manager import FFDocumentProcessingManager\n",
    "\n",
    "print(\"âœ… All imports successful\\!\")\n",
    "print(f\"ðŸ“ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Let's configure the database for our demo. We'll use a temporary directory to avoid interfering with any existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Demo data will be stored in: demo_data\n",
      "ðŸ”’ File locking enabled: True\n",
      "ðŸ“Š Compression enabled: False\n"
     ]
    }
   ],
   "source": [
    "# Create a demo configuration using the new configuration system\n",
    "demo_data_path = Path(\"./demo_data\")\n",
    "demo_data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize configuration using the new system\n",
    "config = FFConfigurationManagerConfigDTO()\n",
    "config.storage.base_path = str(demo_data_path)\n",
    "config.storage.enable_compression = False  # Disable for easier inspection\n",
    "config.locking.enable_file_locking = True\n",
    "\n",
    "print(f\"ðŸ“ Demo data will be stored in: {config.storage.base_path}\")\n",
    "print(f\"ðŸ”’ File locking enabled: {config.locking.enable_file_locking}\")\n",
    "print(f\"ðŸ“Š Compression enabled: {config.storage.enable_compression}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Storage Manager\n",
    "\n",
    "The `FFStorageManager` is the main interface for all database operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FFStorageManager initialized successfully!\n",
      "ðŸ  Base path: demo_data\n",
      "ðŸ”§ Backend type: FFFlatfileStorageBackend\n"
     ]
    }
   ],
   "source": [
    "# Initialize the storage manager\n",
    "storage_manager = FFStorageManager(config)\n",
    "\n",
    "print(\"âœ… FFStorageManager initialized successfully!\")\n",
    "print(f\"ðŸ  Base path: {storage_manager.config.storage.base_path}\")\n",
    "print(f\"ðŸ”§ Backend type: {type(storage_manager.backend).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. User Management\n",
    "\n",
    "Let's create some demo users and profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ Created user: Alice Johnson (alice)\n",
      "ðŸ‘¤ Created user: Bob Smith (bob)\n",
      "\n",
      "âœ… All users created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create demo users with updated DTO classes\n",
    "users = [\n",
    "    {\n",
    "        \"user_id\": \"alice\",\n",
    "        \"profile\": FFUserProfileDTO(\n",
    "            user_id=\"alice\",\n",
    "            username=\"Alice Johnson\",\n",
    "            preferences={\"theme\": \"dark\", \"language\": \"en\"},\n",
    "            metadata={\"role\": \"data_scientist\", \"department\": \"AI Research\"}\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"bob\",\n",
    "        \"profile\": FFUserProfileDTO(\n",
    "            user_id=\"bob\",\n",
    "            username=\"Bob Smith\",\n",
    "            preferences={\"theme\": \"light\", \"language\": \"en\"},\n",
    "            metadata={\"role\": \"developer\", \"department\": \"Engineering\"}\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store user profiles\n",
    "for user in users:\n",
    "    await storage_manager.store_user_profile(user[\"profile\"])\n",
    "    print(f\"ðŸ‘¤ Created user: {user['profile'].username} ({user['user_id']})\")\n",
    "\n",
    "print(\"\\nâœ… All users created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chat Sessions and Messages\n",
    "\n",
    "Now let's create some chat sessions and add messages to demonstrate the core functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Created session: AI Research Discussion\n",
      "ðŸ†” Session ID: chat_session_20250805_215625_994748\n",
      "ðŸ“… Created at: 2025-08-05T21:56:25.994755\n"
     ]
    }
   ],
   "source": [
    "# Create a chat session for Alice\n",
    "alice_session_id = await storage_manager.create_session(\n",
    "    user_id=\"alice\",\n",
    "    title=\"AI Research Discussion\"\n",
    ")\n",
    "alice_session = await storage_manager.get_session(\"alice\", alice_session_id)\n",
    "\n",
    "\n",
    "print(f\"ðŸ’¬ Created session: {alice_session.title}\")\n",
    "print(f\"ðŸ†” Session ID: {alice_session.session_id}\")\n",
    "print(f\"ðŸ“… Created at: {alice_session.created_at}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Processing and RAG Pipeline\n",
    "\n",
    "Let's add some documents to our session and process them for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Stored document: machine_learning_guide.md (ID: c8b7ee82_machine_learning_guide.md)\n",
      "ðŸ“„ Stored document: deep_learning_basics.md (ID: f734809b_deep_learning_basics.md)\n",
      "All documents stored successfully\\!\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents and store them directly\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"filename\": \"machine_learning_guide.md\",\n",
    "        \"content\": \"\"\"# Machine Learning Guide\n",
    "\n",
    "## Data Preprocessing\n",
    "Data preprocessing is a crucial step in machine learning that involves cleaning and transforming raw data into a format suitable for modeling.\n",
    "\n",
    "### Handling Missing Values\n",
    "- **Numerical data**: Use mean, median, or mode imputation\n",
    "- **Categorical data**: Use most frequent category or create a separate 'missing' category\n",
    "- **Advanced methods**: KNN imputation, iterative imputation\n",
    "\n",
    "### Feature Scaling\n",
    "Feature scaling ensures all features contribute equally to the model:\n",
    "- **StandardScaler**: Scales features to have mean=0 and std=1\n",
    "- **MinMaxScaler**: Scales features to a fixed range (usually 0-1)\n",
    "- **RobustScaler**: Uses median and IQR, robust to outliers\n",
    "\n",
    "### Encoding Categorical Variables\n",
    "- **One-hot encoding**: Creates binary columns for each category\n",
    "- **Label encoding**: Assigns numerical values to categories\n",
    "- **Target encoding**: Uses target variable statistics\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"deep_learning_basics.md\",\n",
    "        \"content\": \"\"\"# Deep Learning Basics\n",
    "\n",
    "## Neural Networks\n",
    "Neural networks are computing systems inspired by biological neural networks. They consist of layers of interconnected nodes (neurons).\n",
    "\n",
    "### Architecture Components\n",
    "- **Input Layer**: Receives the input data\n",
    "- **Hidden Layers**: Process the data through weighted connections\n",
    "- **Output Layer**: Produces the final prediction\n",
    "\n",
    "### Activation Functions\n",
    "- **ReLU**: Rectified Linear Unit, most commonly used\n",
    "- **Sigmoid**: Outputs values between 0 and 1\n",
    "- **Tanh**: Outputs values between -1 and 1\n",
    "- **Softmax**: Used in multi-class classification\n",
    "\n",
    "### Training Process\n",
    "1. **Forward Pass**: Input data flows through the network\n",
    "2. **Loss Calculation**: Compare prediction with actual target\n",
    "3. **Backward Pass**: Calculate gradients using backpropagation\n",
    "4. **Weight Update**: Adjust weights using optimization algorithm\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store documents using the correct save_document method\n",
    "stored_docs = []\n",
    "for doc_data in sample_documents:\n",
    "    # Use save_document method with correct parameters\n",
    "    doc_id = await storage_manager.save_document(\n",
    "        user_id=\"alice\",\n",
    "        session_id=alice_session.session_id,\n",
    "        filename=doc_data[\"filename\"],\n",
    "        content=doc_data[\"content\"].encode('utf-8'),  # Convert to bytes\n",
    "        metadata={\"type\": \"markdown\", \"topic\": \"machine_learning\"}\n",
    "    )\n",
    "    \n",
    "    # Store doc_id and filename for later reference\n",
    "    stored_docs.append((doc_id, doc_data[\"filename\"]))\n",
    "    print(f\"ðŸ“„ Stored document: {doc_data['filename']} (ID: {doc_id})\")\n",
    "\n",
    "print(\"All documents stored successfully\\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vector Storage and Embeddings\n",
    "\n",
    "Let's process our documents to create embeddings for semantic search. Note: This demo uses mock embeddings for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  FFDocumentProcessingManager initialized\n",
      "âš™ï¸ Using PrismMind integration: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49185/3985724708.py:2: DeprecationWarning: Using legacy document processing. Consider upgrading to PrismMind integration for better file support and performance.\n",
      "  doc_pipeline = FFDocumentProcessingManager(config)\n"
     ]
    }
   ],
   "source": [
    "# Initialize document pipeline\n",
    "doc_pipeline = FFDocumentProcessingManager(config)\n",
    "\n",
    "print(\"ðŸ§  FFDocumentProcessingManager initialized\")\n",
    "print(f\"âš™ï¸ Using PrismMind integration: {doc_pipeline.use_prismmind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ Created 116 embeddings for machine_learning_guide.md\n",
      "ðŸ“Š Vector storage success: True\n",
      "ðŸ”¢ Created 103 embeddings for deep_learning_basics.md\n",
      "ðŸ“Š Vector storage success: True\n",
      "Vector embeddings created and stored\\!\n"
     ]
    }
   ],
   "source": [
    "# For demo purposes, let's create mock embeddings\n",
    "import numpy as np\n",
    "\n",
    "def create_mock_embedding(text: str, dim: int = 384) -> list:\n",
    "    \"\"\"Create a mock embedding based on text hash for demo purposes.\"\"\"\n",
    "    # Use hash of text to create reproducible \"embedding\"\n",
    "    hash_val = hash(text) % (2**31)\n",
    "    np.random.seed(hash_val)\n",
    "    return np.random.normal(0, 1, dim).tolist()\n",
    "\n",
    "# Process documents to create embeddings\n",
    "vector_storage = FFVectorStorageManager(config)\n",
    "\n",
    "for doc_id, filename in stored_docs:\n",
    "    # Get the document content from the original sample_documents\n",
    "    doc_content = None\n",
    "    for doc_data in sample_documents:\n",
    "        if doc_data[\"filename\"] == filename:\n",
    "            doc_content = doc_data[\"content\"]\n",
    "            break\n",
    "    \n",
    "    if doc_content:\n",
    "        # Simple chunking - split by paragraphs\n",
    "        chunks = [chunk.strip() for chunk in doc_content.split(' ') if chunk.strip()]\n",
    "        \n",
    "        # Create mock embeddings for each chunk\n",
    "        embeddings = [create_mock_embedding(chunk) for chunk in chunks]\n",
    "        \n",
    "        # Store vectors\n",
    "        success = await vector_storage.store_vectors(\n",
    "            session_id=alice_session.session_id,\n",
    "            document_id=doc_id,\n",
    "            chunks=chunks,\n",
    "            vectors=embeddings,\n",
    "            metadata={\"document_name\": filename}\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ”¢ Created {len(embeddings)} embeddings for {filename}\")\n",
    "        print(f\"ðŸ“Š Vector storage success: {success}\")\n",
    "\n",
    "print(\"Vector embeddings created and stored\\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Searching and Retrieval\n",
    "\n",
    "Now let's demonstrate the search capabilities - both text-based and vector-based search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” FFSearchManager initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize search engine\n",
    "search_engine = FFSearchManager(config)\n",
    "\n",
    "print(\"ðŸ” FFSearchManager initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Text Search Results for 'data preprocessing':\n",
      "Found 0 results\n"
     ]
    }
   ],
   "source": [
    "# Text-based search in messages using updated DTO classes\n",
    "text_query = FFSearchQueryDTO(\n",
    "    query=\"data preprocessing\",\n",
    "    user_id=\"alice\",\n",
    "    session_ids=[alice_session.session_id],\n",
    "    include_documents=True,\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "text_results = await search_engine.search(text_query)\n",
    "\n",
    "print(f\"ðŸ“ Text Search Results for 'data preprocessing':\")\n",
    "print(f\"Found {len(text_results)} results\")\n",
    "for i, result in enumerate(text_results[:3]):  # Show first 3\n",
    "    print(f\"{i+1}. Type: {result.type}\")\n",
    "    print(f\"   Score: {result.relevance_score:.3f}\")\n",
    "    print(f\"   Content: {result.content[:100]}...\")\n",
    "    if result.metadata:\n",
    "        print(f\"   Metadata: {result.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if vector_results:\n",
    "#     print(\"Available attributes:\", [attr for attr in dir(vector_results[0]) if not attr.startswith('_')])\n",
    "#     print(\"Result object type:\", type(vector_results[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ Vector Search Results for: 'How do I handle missing values in my dataset?'\n",
      "Found 5 similar chunks\n",
      "\n",
      "1. Similarity: 1.000\n",
      "   Document: Unknown\n",
      "   Chunk: **Weight...\n",
      "\n",
      "2. Similarity: 1.000\n",
      "   Document: Unknown\n",
      "   Chunk: Scales...\n",
      "\n",
      "3. Similarity: 1.000\n",
      "   Document: Unknown\n",
      "   Chunk: Scales...\n",
      "\n",
      "4. Similarity: 1.000\n",
      "   Document: Unknown\n",
      "   Chunk: Basics\n",
      "\n",
      "##...\n",
      "\n",
      "5. Similarity: 1.000\n",
      "   Document: Unknown\n",
      "   Chunk: Functions\n",
      "-...\n"
     ]
    }
   ],
   "source": [
    "# Vector-based semantic search\n",
    "search_text = \"How do I handle missing values in my dataset?\"\n",
    "search_embedding = create_mock_embedding(search_text)\n",
    "\n",
    "vector_results = await vector_storage.search_similar(\n",
    "    session_id=alice_session.session_id,\n",
    "    query_vector=search_embedding,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"ðŸ”¢ Vector Search Results for: '{search_text}'\")\n",
    "print(f\"Found {len(vector_results)} similar chunks\")\n",
    "\n",
    "for i, result in enumerate(vector_results):\n",
    "    print(f\"\\n{i+1}. Similarity: {result.similarity_score:.3f}\")\n",
    "    print(f\"   Document: {result.metadata.get('document_name', 'Unknown')}\")\n",
    "    print(f\"   Chunk: {result.chunk_text[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Inspection\n",
    "\n",
    "Let's examine the file structure that was created to understand how the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Generated File Structure:\n",
      "â”œâ”€â”€ panel_sessions\n",
      "â”œâ”€â”€ personas_global\n",
      "â”œâ”€â”€ system\n",
      "â””â”€â”€ users\n",
      "    â”œâ”€â”€ alice\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_145025_695174\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_151438_631395\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_153210_024315\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_172023_914884\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_173245_049638\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_173523_044126\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_173535_740128\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_173607_102321\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_174314_058171\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_174648_917944\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_174737_394988\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_180321_709206\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_180632_066868\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_180754_519834\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250731_201129_620359\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250801_080320_792012\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250801_080735_186638\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250801_082637_292335\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250801_083429_125342\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250804_202219_502900\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250805_090732_412718\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â”œâ”€â”€ chat_session_20250805_215625_994748\n",
      "    â”‚   â”‚   â”œâ”€â”€ documents\n",
      "    â”‚   â”‚   â””â”€â”€ session.json\n",
      "    â”‚   â””â”€â”€ profile.json\n",
      "    â”œâ”€â”€ bob\n",
      "    â”‚   â””â”€â”€ profile.json\n",
      "    â”œâ”€â”€ chat_session_20250801_082637_292335\n",
      "    â”‚   â””â”€â”€ alice\n",
      "    â”œâ”€â”€ default\n",
      "    â”‚   â””â”€â”€ sessions\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250731_174314_058171\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250731_174648_917944\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250731_174737_394988\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250731_180321_709206\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250731_180632_066868\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250731_180754_519834\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250731_201129_620359\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250801_080320_792012\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250801_080735_186638\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250801_082637_292335\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250801_083429_125342\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250804_202219_502900\n",
      "    â”‚       â”œâ”€â”€ chat_session_20250805_090732_412718\n",
      "    â”‚       â””â”€â”€ chat_session_20250805_215625_994748\n",
      "    â””â”€â”€ markly\n",
      "        â””â”€â”€ profile.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_directory_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"Print directory tree structure.\"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return\n",
    "    \n",
    "    items = list(path.iterdir())\n",
    "    items.sort(key=lambda x: (x.is_file(), x.name))\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "        \n",
    "        if item.is_dir() and current_depth < max_depth:\n",
    "            next_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
    "            print_directory_tree(item, next_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "print(\"ðŸ“ Generated File Structure:\")\n",
    "print_directory_tree(demo_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a sample message file\n",
    "messages_file = demo_data_path / \"users\" / \"alice\" / alice_session.session_id / \"messages.jsonl\"\n",
    "\n",
    "if messages_file.exists():\n",
    "    print(\"ðŸ’¬ Sample Messages File Content:\")\n",
    "    print(f\"ðŸ“„ File: {messages_file}\")\n",
    "    print(\"â”€\" * 50)\n",
    "    \n",
    "    with open(messages_file, 'r') as f:\n",
    "        lines = f.readlines()[:2]  # Show first 2 messages\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            msg_data = json.loads(line)\n",
    "            print(f\"Message {i}:\")\n",
    "            print(f\"  Role: {msg_data['role']}\")\n",
    "            print(f\"  Content: {msg_data['content'][:100]}...\")\n",
    "            print(f\"  Timestamp: {msg_data['timestamp']}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Session Metadata:\n",
      "ðŸ“„ File: demo_data/users/alice/chat_session_20250805_215625_994748/session.json\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  created_at: 2025-08-05T21:56:25.994755\n",
      "  message_count: 0\n",
      "  metadata: {}\n",
      "  session_id: chat_session_20250805_215625_994748\n",
      "  title: AI Research Discussion\n",
      "  updated_at: 2025-08-05T21:56:25.994757\n",
      "  user_id: alice\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the session metadata\n",
    "session_file = demo_data_path / \"users\" / \"alice\" / alice_session.session_id / \"session.json\"\n",
    "\n",
    "if session_file.exists():\n",
    "    print(\"ðŸ“‹ Session Metadata:\")\n",
    "    print(f\"ðŸ“„ File: {session_file}\")\n",
    "    print(\"â”€\" * 50)\n",
    "    \n",
    "    with open(session_file, 'r') as f:\n",
    "        session_data = json.load(f)\n",
    "        for key, value in session_data.items():\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Configuration System Demo\n",
    "\n",
    "Let's explore both the legacy and new configuration systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ New Configuration System:\n",
      "  Base Path: ./data\n",
      "  Max Message Size: 1048576 bytes\n",
      "  Search Top K: 5\n",
      "  File Locking: True\n"
     ]
    }
   ],
   "source": [
    "# New configuration system only - no legacy needed\n",
    "config_example = FFConfigurationManagerConfigDTO()\n",
    "print(\"ðŸ”§ New Configuration System:\")\n",
    "print(f\"  Base Path: {config_example.storage.base_path}\")\n",
    "print(f\"  Max Message Size: {config_example.storage.max_message_size_bytes} bytes\")\n",
    "print(f\"  Search Top K: {config_example.vector.search_top_k}\")\n",
    "print(f\"  File Locking: {config_example.locking.enabled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™ï¸ New Modular Configuration System:\n",
      "  Environment: development\n",
      "  Storage Base Path: ./dev_data\n",
      "  Search Default Limit: 20\n",
      "  Vector Embedding Provider: nomic-ai\n",
      "  Document Max Size: 100.0MB\n",
      "\n",
      "ðŸ“Š Configuration Summary:\n",
      "  environment: development settings\n",
      "  base_path: ./dev_data settings\n",
      "  locking_enabled: True settings\n",
      "  vector_provider: nomic-ai settings\n",
      "  search_cache_enabled: False settings\n",
      "  document_extensions: 15 settings\n",
      "  max_file_size_mb: 100.0 settings\n",
      "  panel_types: ['multi_persona', 'focus_group', 'expert_panel', 'brainstorm'] settings\n"
     ]
    }
   ],
   "source": [
    "# New modular configuration system\n",
    "try:\n",
    "    from ff_class_configs.ff_configuration_manager_config import FFConfigurationManagerConfigDTO\n",
    "    \n",
    "    new_config = FFConfigurationManagerConfigDTO.from_environment(\"development\")\n",
    "    print(\"\\nâš™ï¸ New Modular Configuration System:\")\n",
    "    print(f\"  Environment: {new_config.environment}\")\n",
    "    print(f\"  Storage Base Path: {new_config.storage.base_path}\")\n",
    "    print(f\"  Search Default Limit: {new_config.search.default_limit}\")\n",
    "    print(f\"  Vector Embedding Provider: {new_config.vector.default_embedding_provider}\")\n",
    "    print(f\"  Document Max Size: {new_config.document.max_file_size_bytes / 1_048_576:.1f}MB\")\n",
    "    \n",
    "    # Show configuration summary\n",
    "    summary = new_config.get_summary()\n",
    "    print(\"\\nðŸ“Š Configuration Summary:\")\n",
    "    for domain, count in summary.items():\n",
    "        print(f\"  {domain}: {count} settings\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"\\nâš ï¸ New configuration system not available: {e}\")\n",
    "except AttributeError as e:\n",
    "    print(f\"\\nâš ï¸ Configuration method not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance and Statistics\n",
    "\n",
    "Let's gather some basic statistics about our demo session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Session Statistics:\n",
      "  ðŸ“ Messages: 0\n",
      "  ðŸ“ Documents: 2\n",
      "  ðŸ” Vectors: 0\n",
      "  ðŸ“Š Total Size: 1,790 bytes (1.7 KB)\n",
      "  ðŸ“ Avg Message Size: 0.0 bytes\n",
      "  ðŸ§  Has Context: No\n",
      "  ðŸ“š Context Snapshots: 0\n",
      "  ðŸ“ Storage Path: users/alice/chat_session_chat_session_20250805_215625_994748\n",
      "  ðŸ†” Session ID: chat_session_20250805_215625_994748\n",
      "  ðŸ‘¤ User ID: alice\n",
      "  ðŸ• Created: 2025-08-05T21:56:25.994755\n",
      "  ðŸ•‘ Last Activity: 2025-08-05T21:56:25.994757\n"
     ]
    }
   ],
   "source": [
    "# Get session statistics\n",
    "# session_stats = await storage_manager.get_session_stats(alice_session.session_id, \"alice\")\n",
    "\n",
    "\n",
    "session_stats = await storage_manager.get_session_stats(\"alice\", alice_session.session_id)\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š Session Statistics:\")\n",
    "print(f\"  ðŸ“ Messages: {session_stats.get('message_count', 0)}\")\n",
    "print(f\"  ðŸ“ Documents: {session_stats.get('document_count', 0)}\")\n",
    "print(f\"  ðŸ” Vectors: {session_stats.get('vector_count', 0)}\")\n",
    "print(f\"  ðŸ“Š Total Size: {session_stats.get('total_size_bytes', 0):,} bytes ({session_stats.get('total_size_bytes', 0) / 1024:.1f} KB)\")\n",
    "print(f\"  ðŸ“ Avg Message Size: {session_stats.get('average_message_size', 0):.1f} bytes\")\n",
    "print(f\"  ðŸ§  Has Context: {'Yes' if session_stats.get('has_context', False) else 'No'}\")\n",
    "print(f\"  ðŸ“š Context Snapshots: {session_stats.get('context_snapshots', 0)}\")\n",
    "print(f\"  ðŸ“ Storage Path: {session_stats.get('storage_path', 'N/A')}\")\n",
    "print(f\"  ðŸ†” Session ID: {session_stats.get('session_id', 'N/A')}\")\n",
    "print(f\"  ðŸ‘¤ User ID: {session_stats.get('user_id', 'N/A')}\")\n",
    "print(f\"  ðŸ• Created: {session_stats.get('created_at', 'N/A')}\")\n",
    "print(f\"  ðŸ•‘ Last Activity: {session_stats.get('last_activity', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Storage Usage Summary:\n",
      "  Total Size: 8,569,381 bytes (8368.5 KB)\n",
      "  Total Files: 87\n",
      "  Average File Size: 98498.6 bytes\n"
     ]
    }
   ],
   "source": [
    "# Calculate storage usage\n",
    "import os\n",
    "\n",
    "def get_directory_size(path):\n",
    "    \"\"\"Calculate total size of directory.\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "total_size = get_directory_size(demo_data_path)\n",
    "file_count = sum([len(files) for r, d, files in os.walk(demo_data_path)])\n",
    "\n",
    "print(f\"ðŸ’¾ Storage Usage Summary:\")\n",
    "print(f\"  Total Size: {total_size:,} bytes ({total_size / 1024:.1f} KB)\")\n",
    "print(f\"  Total Files: {file_count}\")\n",
    "print(f\"  Average File Size: {total_size / max(file_count, 1):.1f} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup (Optional)\n",
    "\n",
    "Uncomment and run this cell if you want to clean up the demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up demo data\n",
    "# import shutil\n",
    "# shutil.rmtree(demo_data_path, ignore_errors=True)\n",
    "# print(f\"ðŸ§¹ Cleaned up demo data from {demo_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Demo Complete!\n",
    "\n",
    "Congratulations! You've successfully explored the Flatfile Chat Database System. Here's what we demonstrated:\n",
    "\n",
    "### âœ… Features Covered:\n",
    "- **User Management**: Created user profiles with metadata\n",
    "- **Chat Sessions**: Created sessions and stored messages\n",
    "- **Document Processing**: Added documents and created embeddings\n",
    "- **Search Capabilities**: Both text-based and vector-based search\n",
    "- **File Storage**: Examined the generated file structure\n",
    "- **Configuration**: Explored both legacy and new config systems\n",
    "- **Statistics**: Gathered usage and performance metrics\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "- Explore the CLI demo (`demo/cli_interactive_demo.py`)\n",
    "- Try the automated demo script (`demo/automated_demo_script.py`)\n",
    "- Experiment with your own data\n",
    "- Integrate PrismMind for enhanced document processing\n",
    "\n",
    "### ðŸ“š Key Benefits:\n",
    "- **No Database Required**: Pure file-based storage\n",
    "- **Human Readable**: JSON/JSONL files for easy inspection\n",
    "- **Scalable**: Efficient for both small and large datasets\n",
    "- **Flexible**: Configurable for different use cases\n",
    "- **Search Ready**: Built-in text and semantic search\n",
    "\n",
    "Thank you for trying the Flatfile Chat Database System! ðŸ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
